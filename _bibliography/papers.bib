---
---

@string{aps = {American Physical Society,}}

@inproceedings{10.1145/3572848.3577476,
  author = {Jiang, Jiantong and Wen, Zeyi and Mansoor, Atif and Mian, Ajmal},
  title = {Fast Parallel Exact Inference on Bayesian Networks},
  year = {2023},
  abstract = {Bayesian networks (BNs) are attractive, because they are graphical and interpretable machine learning models. However, exact inference on BNs is time-consuming, especially for complex problems. To improve the efficiency, we propose a fast BN exact inference solution named Fast-BNI on multi-core CPUs. Fast-BNI enhances the efficiency of exact inference through hybrid parallelism that tightly integrates coarse- and fine-grained parallelism. We also propose techniques to further simplify the bottleneck operations of BN exact inference. Fast-BNI source code is freely available at https://github.com/jjiantong/FastBN.},
  booktitle = {ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (PPoPP)},
  pages = {425–426},
  series = {PPoPP '23},

  bibtex_show = {true},
  preview = {ppopp23_bnei.jpg},
  paper = {ppopp23-bnei.pdf},
  poster = {ppopp23_bnei.pdf},
  code = {https://github.com/jjiantong/FastBN}
}



@INPROCEEDINGS{9820657,
  author={Jiang, Jiantong and Wen, Zeyi and Mian, Ajmal},
  booktitle={IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={Fast Parallel Bayesian Network Structure Learning}, 
  year={2022},
  abstract = {Bayesian networks (BNs) are a widely used graphical model in machine learning for representing knowledge with uncertainty. The mainstream BN structure learning methods require performing a large number of conditional independence (CI) tests. The learning process is very time-consuming, especially for high-dimensional problems, which hinders the adoption of BNs to more applications. Existing works attempt to accelerate the learning process with parallelism, but face issues including load unbalancing, costly atomic operations and dominant parallel overhead. In this paper, we propose a fast solution named FastBNS on multi-core CPUs to enhance the efficiency of the BN structure learning. Fast-BNS is powered by a series of efficiency optimizations including (i) designing a dynamic work pool to monitor the processing of edges and to better schedule the workloads among threads, (ii) grouping the CI tests of the edges with the same endpoints to reduce the number of unnecessary CI tests, (iii) using a cache-friendly data storage to improve the memory efficiency, and (iv) generating the conditioning sets onthe-fly to avoid extra memory consumption. A comprehensive experimental study shows that the sequential version of FastBNS is up to 50 times faster than its counterpart, and the parallel version of Fast-BNS achieves 4.8 to 24.5 times speedup over the state-of-the-art multi-threaded solution. Moreover, Fast-BNS has a good scalability to the network size as well as sample size.},
  pages={617-627},

  bibtex_show = {true},
  preview = {ipdps22_bnsl.jpg},
  paper = {ipdps22-bnsl.pdf},
  slides = {ipdps22_bnsl.pdf},
  code = {https://github.com/jjiantong/FastBN},
  video = {https://www.bilibili.com/video/BV1Ra411R71R/?spm_id_from=333.999.0.0}
}


@article{jiang2021parallel,
  title={Parallel and distributed structured SVM training},
  author={Jiang, Jiantong and Wen, Zeyi and Wang, Zeke and He, Bingsheng and Chen, Jian},
  journal={IEEE Transactions on Parallel and Distributed Systems (TPDS)},
  abstract = {Structured Support Vector Machines (structured SVMs) are a fundamental machine learning algorithm, and have solid theoretical foundation and high effectiveness in applications such as natural language parsing and computer vision. However, training structured SVMs is very time-consuming, due to the large number of constraints and inferior convergence rates, especially for large training data sets. The high cost of training structured SVMs has hindered its adoption to new applications. In this article, we aim to improve the efficiency of structured SVMs by proposing a parallel and distributed solution (namely FastSSVM) for training structured SVMsbuilding on top of MPI and OpenMP. FastSSVMexploits a series of optimizations (e.g., optimizations on data storage and synchronization) to efficiently use the resources of the nodes in a cluster and the cores of the nodes. Moreover, FastSSVM tackles the large constraint set problem by batch processing and addresses the slow convergence challenge by adapting stop conditions based on the improvement of each iteration. We theoretically prove that our solution is guaranteed to converge to a global optimum. A comprehensive experimental study shows that FastSSVM can achieve at least four times speedup over the existing solutions, and in somecasescan achieve twoto three orders of magnitude speedup.},
  volume={33},
  number={5},
  pages={1084--1096},
  year={2021},
  publisher={IEEE},

  bibtex_show = {true},
  preview = {tpds21_ssvm.jpg},
  paper = {tpds21-ssvm.pdf}
}


@inproceedings{10.1145/3373087.3375313,
  author = {Jiang, Jiantong and Wang, Zeke and Liu, Xue and G\'{o}mez-Luna, Juan and Guan, Nan and Deng, Qingxu and Zhang, Wei and Mutlu, Onur},
  title = {Boyi: A Systematic Framework for Automatically Deciding the Right Execution Model of OpenCL Applications on FPGAs},
  year = {2020},
  abstract = {FPGA vendors provide OpenCL software development kits for easier programmability, with the goal of replacing the time-consuming and error-prone register-transfer level (RTL) programming. Many studies explore optimization methods (e.g., loop unrolling, local memory) to accelerate OpenCL programs running on FPGAs. These programs typically follow the default OpenCL execution model, where a kernel deploys multiple work-items arranged into work-groups. However, the default execution model is not always a good fit for an application mapped to the FPGA architecture, which is very different from the multithreaded architecture of GPUs, for which OpenCL was originally designed. In this work, we identify three other execution models that can better utilize the FPGA resources for the OpenCL applications that do not fit well into the default execution model. These three execution models are based on two OpenCL features devised for FPGA programming (namely, single work-item kernel and OpenCL channel). We observe that the selection of the right execution model determines the performance upper bound of a particular application, which can vary by two orders magnitude between the most suitable execution model and the most unsuitable one. However, there is no way to select the most suitable execution model other than empiricall exploring the optimization space for the four of them, which can be prohibitive. To help FPGA programmers identify the right execution model, we propose Boyi, a systematic framework that makes automatic decisions by analyzing OpenCL programming patterns in an application. After finding the right execution model with the help of Boyi, programmers can apply other conventional optimizations to reach the performance upper bound. Our experimental evaluation shows that Boyi can 1) accurately determine the right execution model, and 2) greatly reduce the exploration space of conventional optimization methods.},
  booktitle = {ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)},
  pages = {299–309},
  series = {FPGA '20},

  bibtex_show = {true},
  preview = {fpga20_boyi.jpg},
  paper = {fpga20-boyi.pdf},
  slides = {fpga20_boyi.pdf},
  code = {https://github.com/jjiantong/Boyi}
}